Very basic: Law of Large Numbers and Central Limit Theorem (In this repository I only discuss the frequentist approach of statistics).

Asymptopia is extremely important for an estimator to be consistent. Asymptopia means that the estimator monotonically converges to one value as the size of data approaches infinity, and it helps identifying the parameter that we are interested in uncovering. Let me give an example here. A typical regularity that applies the asymptotic theory is the law of large numbers which states that for a sequence of independently and identically distributed (aka i.i.d.) random variables X1, X2, ..., the sample averages <a href="http://www.codecogs.com/eqnedit.php?latex=\bar{X_i}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\bar{X_i}" title="\bar{X_i}" /></a> converge in probability to the population mean E[Xi] 

Central Limit Theorem gives us an asymptotic distribution. It states that "the arithmetic mean of a sufficiently large number of iterates of independent random variables, each with a well-defined expected value and well-defined variance, will be approximately normally distributed" (Rice, John (1995), Mathematical Statistics and Data Analysis (Second ed.), Duxbury Press, ISBN 0-534-20934-3)). Note that the resulting normal distribution does not depend on the distribution of the population from which each sample is drawn, which allows us to conveniently operate on a readily available estimator, <a href="http://www.codecogs.com/eqnedit.php?latex=\bar{X_i}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\bar{X_i}" title="\bar{X_i}" /></a> to carry out statistical analysis. In the two graphs that I created, you'll see the nice property of sample means that CLT gives us.
